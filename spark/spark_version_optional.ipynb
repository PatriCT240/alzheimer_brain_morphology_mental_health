{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032027e1-a620-4b5b-93c3-8c774b7b8ae7",
   "metadata": {},
   "source": [
    "# Optional Spark Demonstration Notebook\n",
    "\n",
    "This notebook provides an optional, self‑contained demonstration of how the project's data pipeline could be executed using Apache Spark instead of Pandas. It is **not required** to run the main analysis and does **not** replace the primary workflow implemented in the `notebooks/01–07` pipeline.\n",
    "\n",
    "The purpose of this notebook is to demonstrate:\n",
    "\n",
    "- Ability to work with distributed data frameworks (Apache Spark)\n",
    "- Ability to structure data ingestion and transformation pipelines in a scalable environment\n",
    "- Ability to integrate Spark into a modular project architecture\n",
    "- Ability to reproduce a subset of the Alzheimer’s dataset preprocessing workflow using Spark DataFrames\n",
    "\n",
    "This notebook is intentionally minimal and focused.  \n",
    "It mirrors the **first stage** of the project pipeline: *data ingestion and basic preprocessing*, but implemented using Spark instead of Pandas.\n",
    "\n",
    "Do **not** need Spark or Java to evaluate the project.  \n",
    "This notebook exists solely to demonstrate technical breadth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdb5af-c07c-4e49-add6-7ecb32ab164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.functions import floor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e3ec8-1754-4353-a823-cb3c046e53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Java configuration (local execution only)\n",
    "# Adjust this path if your Java 17 installation is located elsewhere\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d55105-0670-461c-93f2-d620a869a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Alzheimer_Spark_Optional\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fae50-337b-4416-81d3-08dc7348a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"data/raw/nacc_alzheimers_dataset.csv\"\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(file_path)\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ae2ea-a819-42b0-a999-af2b50e6ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation aligned with preprocessing pipeline\n",
    "df_clean = (\n",
    "    df\n",
    "    .withColumn(\"age\", col(\"age\").cast(\"integer\"))\n",
    "    .withColumn(\"sex\", when(col(\"sex\") == \"F\", 1).when(col(\"sex\") == \"M\", 0))\n",
    "    .withColumn(\"diagnosis_flag\", when(col(\"diagnosis\") == \"Alzheimer\", 1).otherwise(0))\n",
    "    .filter(col(\"age\").isNotNull())\n",
    ")\n",
    "\n",
    "df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d9436-b86e-437d-8c8a-30dcefa84e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Alzheimer diagnosis rate by age group\n",
    "df_age_groups = (\n",
    "    df_clean\n",
    "    .withColumn(\"age_group\", floor(col(\"age\") / 5) * 5)\n",
    "    .groupBy(\"age_group\")\n",
    "    .agg({\"diagnosis_flag\": \"avg\"})\n",
    "    .orderBy(\"age_group\")\n",
    ")\n",
    "\n",
    "df_age_groups.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b22759-b70f-4f46-8a08-f9f0a1de36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark 17)",
   "language": "python",
   "name": "spark17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
